train_args:
  num_epochs: 5
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 400
  checkpoint: null
  wandb: true
  wandb_project_name: "retriever"
  log_grads_every: 100
  log_lr_every: 10
  save_every: 1000000
  chunk_size: 32
  output_dir: "/shared/nas2/revanth3/query_free_rag/exp/sanity/tuned_query_document/"
  # if using deepspeed, this will be ignored
  schedule_type: "linear"
  max_grad_norm: 1.0
  gradient_accumulation_steps: 2
  deepspeed: true
  deepspeed_config: "configs/deepspeed/ds_config.json"
  adam_beta1: 0.9
  adam_beta2: 0.999
  grad_cache: false
  loss_fn: "clip"
  use_fp8: false
  clamp_logits: false
  logit_max: 100

model_args:
  model_type: "query_document"
  logit_scale: 50
  trainable_logit_scale: false
  add_prefix: true  
  num_negatives: 7 

query_model_args:
  model_type: "query_document"
  seq_len: 128
  pooling: "last"
  tokenizer_name: "Alibaba-NLP/gte-Qwen2-7B-instruct"
  model_name: "Alibaba-NLP/gte-Qwen2-7B-instruct"
  pretrained: true
  gradient_checkpointing: true
  # freeze: true
  projection_dim: 1024
  nomic_encoder: false
  use_fused_kernels: false

document_model_args:
  model_type: "query_document"
  seq_len: 1024
  pooling: "cls"
  tokenizer_name: "Alibaba-NLP/gte-large-en-v1.5"
  model_name: "Alibaba-NLP/gte-large-en-v1.5"
  gradient_checkpointing: true
  pretrained: true
  nomic_encoder: false
  use_fused_kernels: false

data_args:
  input_shards: "configs/data/finetune_triplets_query_document.yaml"
  workers: 8 
  batch_size: 64
  seed: 42
  shuffle: false
  download: true
  streaming: false
  weighted_sampling: false
  verbose: true